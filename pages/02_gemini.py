import streamlit as st
from streamlit_webrtc import webrtc_streamer, WebRtcMode, AudioProcessorBase
import numpy as np
import whisper
import io
from pydub import AudioSegment
import os
import tempfile

# Whisper ëª¨ë¸ ë¡œë“œ (ì•± ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ë¡œë“œí•˜ë„ë¡ ìºì‹±)
@st.cache_resource
def load_model():
    # ëª¨ë¸ í¬ê¸°ëŠ” "tiny", "base", "small", "medium", "large" ì¤‘ì—ì„œ ì„ íƒ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    # "base" ëª¨ë¸ì´ ì†ë„ì™€ ì •í™•ë„ ë©´ì—ì„œ ì ì ˆí•œ ì‹œì‘ì ì…ë‹ˆë‹¤.
    model = whisper.load_model("base")
    return model

model = load_model()

st.title("ğŸ¤ ì‹¤ì‹œê°„ ìŒì„±-í…ìŠ¤íŠ¸ ë³€í™˜ ì•±")
st.write("ì•„ë˜ 'START' ë²„íŠ¼ì„ ëˆ„ë¥´ê³  ë§ˆì´í¬ì— ë§í•˜ë©´ ì‹¤ì‹œê°„ìœ¼ë¡œ í…ìŠ¤íŠ¸ê°€ ë³€í™˜ë©ë‹ˆë‹¤.")

# webrtc ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ì˜¤ë””ì˜¤ ì²˜ë¦¬ í´ë˜ìŠ¤
class AudioProcessor(AudioProcessorBase):
    def __init__(self):
        self.audio_buffer = io.BytesIO()

    # webrtcë¡œë¶€í„° ì˜¤ë””ì˜¤ í”„ë ˆì„ì„ ë°›ìŠµë‹ˆë‹¤.
    def recv(self, frame):
        # pydubì—ì„œ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ numpy ë°°ì—´ì„ AudioSegmentë¡œ ë³€í™˜
        sound = AudioSegment(
            data=frame.to_ndarray().tobytes(),
            sample_width=frame.format.bytes,
            frame_rate=frame.sample_rate,
            channels=len(frame.layout.channels),
        )
        # ë°›ì€ ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ë²„í¼ì— ëˆ„ì 
        self.audio_buffer.write(sound.raw_data)
        
        return frame

# webrtc_streamer ì»´í¬ë„ŒíŠ¸ ì„¤ì •
webrtc_ctx = webrtc_streamer(
    key="speech-to-text",
    mode=WebRtcMode.SENDONLY, # ë§ˆì´í¬ ì…ë ¥ë§Œ ë°›ìŒ
    audio_processor_factory=AudioProcessor,
    media_stream_constraints={"video": False, "audio": True},
)

# í…ìŠ¤íŠ¸ ë³€í™˜ ê²°ê³¼ë¥¼ í‘œì‹œí•  ì˜ì—­
result_container = st.empty()
result_container.write("ìŒì„± ì¸ì‹ ëŒ€ê¸° ì¤‘...")

if webrtc_ctx.audio_processor:
    st.write("ë§ˆì´í¬ê°€ ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì œ ë§ì”€í•˜ì„¸ìš”!")

    if st.button("ì§€ê¸ˆê¹Œì§€ì˜ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"):
        # ì˜¤ë””ì˜¤ ë²„í¼ì— ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
        if webrtc_ctx.audio_processor.audio_buffer.getbuffer().nbytes > 0:
            with st.spinner("ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ ì¤‘ì…ë‹ˆë‹¤..."):
                # ë²„í¼ì˜ ì‹œì‘ìœ¼ë¡œ ì´ë™
                webrtc_ctx.audio_processor.audio_buffer.seek(0)
                
                # pydubë¥¼ ì‚¬ìš©í•˜ì—¬ ë²„í¼ì—ì„œ ì˜¤ë””ì˜¤ ë°ì´í„° ë¡œë“œ
                # webrtcì—ì„œ ë°›ì€ raw ì˜¤ë””ì˜¤ ë°ì´í„°ì´ë¯€ë¡œ, í¬ë§· ì •ë³´ë¥¼ ëª…ì‹œí•´ì¤˜ì•¼ í•©ë‹ˆë‹¤.
                audio_segment = AudioSegment.from_raw(
                    webrtc_ctx.audio_processor.audio_buffer,
                    sample_width=2, # 16-bit
                    frame_rate=48000, # webrtcì˜ ê¸°ë³¸ ìƒ˜í”Œë§ ë ˆì´íŠ¸
                    channels=1
                )

                # Whisperê°€ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥ (mp3 í¬ë§· ê¶Œì¥)
                with tempfile.NamedTemporaryFile(suffix=".mp3", delete=False) as tmp:
                    audio_segment.export(tmp.name, format="mp3")
                    tmp_path = tmp.name

                # Whisperë¡œ ìŒì„± ì¸ì‹
                result = model.transcribe(tmp_path, fp16=False)
                
                # ë³€í™˜ ê²°ê³¼ ì¶œë ¥
                result_container.markdown(f"**ë³€í™˜ ê²°ê³¼:**\n> {result['text']}")

                # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                os.remove(tmp_path)
        else:
            result_container.write("ë…¹ìŒëœ ìŒì„±ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë§ˆì´í¬ì— ë§ì”€í•˜ì„¸ìš”.")
else:
    st.write("START ë²„íŠ¼ì„ ëˆŒëŸ¬ ë§ˆì´í¬ ì‚¬ìš©ì„ í—ˆìš©í•´ì£¼ì„¸ìš”.")
