import streamlit as st, queue, time, io, wave
from streamlit_webrtc import webrtc_streamer, AudioProcessorBase
from openai import OpenAI
import av

st.set_page_config(page_title="ğŸ™ï¸ ì‹¤ì‹œê°„ STT (WebRTC)", page_icon="ğŸ™ï¸")
client = OpenAI(api_key=st.secrets["openai_api_key"])
st.title("ğŸ™ï¸ ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹ - WebRTC (ì§€ì—°â‰ˆ0.8 s)")

# â”€â”€â”€ 1) ì˜¤ë””ì˜¤ ìº¡ì²˜ & 16 kHz ë³€í™˜ â”€â”€â”€
class AudioProc(AudioProcessorBase):
    def __init__(self):
        self.buf = bytearray()
        self.last = 0
        self.ph = st.empty()

    def recv_audio(self, frame: av.AudioFrame):
        pcm = frame.reformat(format="s16", layout="mono", rate=16_000)
        self.buf += pcm.planes[0].to_bytes()

        # 1 s(32 kB) ë‹¨ìœ„ë¡œ Whisper í˜¸ì¶œ
        if len(self.buf) >= 32_000 and time.time() - self.last > .7:
            wav = io.BytesIO()
            with wave.open(wav, "wb") as wf:
                wf.setnchannels(1); wf.setsampwidth(2); wf.setframerate(16_000)
                wf.writeframes(self.buf[:32_000])
            wav.seek(0)
            txt = client.audio.transcriptions.create(
                model="whisper-1",
                file=wav,
                language="ko",
                response_format="text",
                temperature=0
            ).strip()
            self.ph.markdown(f"ğŸ“ **{txt}**")
            self.buf.clear(); self.last = time.time()
        return frame

# â”€â”€â”€ 2) WebRTC ìŠ¤íŠ¸ë¦¼ (TURN í¬í•¨) â”€â”€â”€
webrtc_streamer(
    key="stt-webrtc",
    audio_processor_factory=AudioProc,
    media_stream_constraints={"audio": True, "video": False},
    rtc_configuration={
        "iceServers": [
            {"urls": ["stun:stun.l.google.com:19302"]},
            {"urls": ["turn:openrelay.metered.ca:80?transport=tcp"],
             "username": "openrelayproject", "credential": "openrelayproject"}
        ]
    },
    async_processing=True,
)

st.caption("Start ë¥¼ ëˆŒëŸ¬ ë§ˆì´í¬ ê¶Œí•œì„ í—ˆìš©í•˜ë©´ 1 ì´ˆ ë‚´ì™¸ë¡œ ìë§‰ì´ í‘œì‹œë©ë‹ˆë‹¤.")
